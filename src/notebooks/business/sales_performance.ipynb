{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc875887-caa0-4fc0-ac18-cb964259292b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27058014-9788-4a1a-b3f9-d58afe4c49b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dbefd46-ff79-4642-aa05-f15cfaedbf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"pei\"\n",
    "SOURCE_SCHEMA = \"silver\"\n",
    "TARGET_SCHEMA = \"gold\"\n",
    "SOURCE_TABLE_NAME = \"orders_enriched\"\n",
    "TARGET_TABLE_NAME = \"agg_sales_performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3af213-efe0-4d9e-8b99-b99572ad3285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.metadata_manager import get_last_processed_version, update_last_processed_version, get_latest_table_version\n",
    "from data_writers.write_data import upsert_delta_table\n",
    "from data_writers.maintenance import optimize_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be31762-4a6a-4e24-87ae-8d36f6cdc264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# source table latest version\n",
    "latest_table_version = get_latest_table_version(spark, CATALOG, SOURCE_SCHEMA, SOURCE_TABLE_NAME)\n",
    "\n",
    "# source tale last processed version\n",
    "last_processed_version = get_last_processed_version(spark, CATALOG, SOURCE_SCHEMA, SOURCE_TABLE_NAME)\n",
    "\n",
    "# calculate last version\n",
    "last_version = last_processed_version if last_processed_version else -1\n",
    "\n",
    "if last_version >= latest_table_version:\n",
    "    dbutils.notebook.exit\n",
    "    (\n",
    "        f\"Table {TARGET_TABLE_NAME} is already up to date at version {last_version}.\"\n",
    "    )\n",
    "\n",
    "try: \n",
    "    if last_version == -1:\n",
    "        # if full refresh, then read all data from silver table\n",
    "        changes_df = (spark.read.format(\"delta\")\n",
    "                      .option(\"versionAsOf\", latest_table_version)\n",
    "                      .table(f\"{CATALOG}.{SOURCE_SCHEMA}.{SOURCE_TABLE_NAME}\"))\n",
    "    else:\n",
    "        # read incremental changes or updates\n",
    "        changes_df = (spark.read.format(\"delta\")\n",
    "                        .option(\"readChangeFeed\", \"true\")\n",
    "                        .option(\"startingVersion\", last_version + 1)\n",
    "                        .option(\"endingVersion\", latest_table_version) \n",
    "                        .table(f\"{CATALOG}.{SOURCE_SCHEMA}.{SOURCE_TABLE_NAME}\")\n",
    "                        .filter(f.col(\"_change_type\").isin(\"insert\", \"update_postimage\")))\n",
    "        \n",
    "    if changes_df.isEmpty():\n",
    "        update_last_processed_version(spark, CATALOG, SOURCE_SCHEMA, SOURCE_TABLE_NAME, latest_table_version)\n",
    "        dbutils.notebook.exit(\"No changes found, Updated version to latest version\")\n",
    "    \n",
    "    # aggregate on year, category, sub category and customer\n",
    "    df_batch_agg = (\n",
    "        changes_df\n",
    "        .groupBy(\"order_year\", \"category\", \"sub_category\", \"customer_name\")\n",
    "        .agg(f.sum(\"profit\").alias(\"total_profit\"))\n",
    "    )\n",
    "    \n",
    "    # upsert to gold\n",
    "    upsert_delta_table(\n",
    "        spark_session=spark,\n",
    "        df=df_batch_agg,\n",
    "        target_table_name=f\"{CATALOG}.{TARGET_SCHEMA}.{TARGET_TABLE_NAME}\",\n",
    "        join_key=\"order_year,category,sub_category,customer_name\",\n",
    "        partition_col=\"order_year\"\n",
    "    )\n",
    "\n",
    "    # mainatenance script \n",
    "    #optimize_partitions(spark, \n",
    "    #                    f\"{CATALOG}.{TARGET_SCHEMA}.{TARGET_TABLE_NAME}\", \n",
    "    #                    df_batch_agg, \n",
    "    #                    \"order_year\", \n",
    "    #                    \"category,sub_category,customer_name\")\n",
    "\n",
    "    # update table version processed\n",
    "    update_last_processed_version(spark, CATALOG, SOURCE_SCHEMA, SOURCE_TABLE_NAME, latest_table_version)\n",
    "\n",
    "except Exception as e: \n",
    "    print(f\"Pipeline failed. Watermark NOT updated. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sales_performance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
