{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21a75db-375b-4d7e-9db9-c249a5143598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6a21b5-503e-43ea-b572-af3ca3494a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "import pyspark.sql.functions as f \n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from data_loaders.data_load_excel import load_raw_data\n",
    "from utils.file_utils import move_file\n",
    "from data_writers.write_data import write_bronze_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c70d0fe-da15-42a3-b452-79f780f69862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_LANDING_PATH = \"/Volumes/pei/default/landing_volume\"\n",
    "FILE_NAME = \"Customer.xlsx\"\n",
    "TARGET_TABLE = \"pei.bronze.raw_customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0d4b0b-2b71-42d9-a2f6-f066dbba39e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_path = f\"{BASE_LANDING_PATH}/customers/\"\n",
    "processed_path = f\"{BASE_LANDING_PATH}/customers/processed/\"\n",
    "full_path = os.path.join(data_path, FILE_NAME)\n",
    "\n",
    "# check if path exists\n",
    "if os.path.exists(full_path):\n",
    "    try: \n",
    "        # load data\n",
    "        df_customer = load_raw_data(spark_session=spark, file_path=full_path)\n",
    "\n",
    "        # write data\n",
    "        write_bronze_data(\n",
    "            df=df_customer, \n",
    "            target_table=TARGET_TABLE, \n",
    "            is_streaming=False, \n",
    "            file_source_path=full_path, \n",
    "            trigger=None\n",
    "            )\n",
    "       \n",
    "       # move file to processed directory\n",
    "       # TODO: this can lead to dual write problem, if move_file fails and write is succeeded\n",
    "       #  We need to handle this programmatically or we can use autoloader with excel  \n",
    "        move_file(spark, full_path, os.path.join(processed_path, FILE_NAME))\n",
    "    except Exception as e: \n",
    "        print(f\"FAILED: Customers Ingestion (Excel). Error: {str(e)}\")\n",
    "else:\n",
    "    print(f\"SKIPPED: {FILE_NAME} not found in {data_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
